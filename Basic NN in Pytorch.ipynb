{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a module to define layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''Used to define the output of the network'''\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        # .view simply performs a reshape\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # Apply our non-linear transformations. Note, no normalization in output layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the forward operation, `backwards` is automatically defined using autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params)) # This is weights x 5 + biases x 5?\n",
    "print(params[0].size())\n",
    "print(params[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0249  0.0601  0.1530 -0.0804 -0.0951  0.1624 -0.0149 -0.1549  0.0246  0.0109\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32)) # Inputs are Variables\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no concept of a single example input: all inputs have a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0019  0.0837  0.1345 -0.0872 -0.1118  0.1922  0.0327 -0.1203  0.0101  0.0264\n",
      " 0.0026  0.0672  0.1460 -0.0898 -0.0836  0.1745  0.0239 -0.1283  0.0158  0.0238\n",
      " 0.0093  0.0786  0.1222 -0.0836 -0.1114  0.1349  0.0013 -0.1126  0.0274  0.0136\n",
      " 0.0129  0.0640  0.1037 -0.0885 -0.1120  0.1566  0.0100 -0.1213  0.0460  0.0064\n",
      " 0.0075  0.0696  0.1498 -0.0649 -0.1061  0.1430  0.0279 -0.1167  0.0275  0.0397\n",
      "-0.0169  0.0872  0.1331 -0.0714 -0.1036  0.1531 -0.0005 -0.1374  0.0372  0.0090\n",
      " 0.0266  0.0553  0.1458 -0.0767 -0.0857  0.1571  0.0084 -0.1206  0.0267  0.0239\n",
      " 0.0150  0.1086  0.1478 -0.0777 -0.1126  0.1626 -0.0151 -0.1264  0.0134  0.0314\n",
      " 0.0062  0.0858  0.1271 -0.0827 -0.1105  0.1486  0.0137 -0.1179  0.0230  0.0064\n",
      " 0.0159  0.0740  0.1464 -0.0614 -0.1249  0.1153 -0.0037 -0.1509  0.0485  0.0021\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(10, 1, 32, 32)) # Inputs are Variables\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradients and then initialize with random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is defined using the `nn` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.5496\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32)) \n",
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss) # Note that this automatically sums across examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.MSELossBackward at 0x1117fc228>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # The whole graph is differentiated\n",
    "# All the graph variables now have their .grad updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,.,.) = \n",
       " -0.1817  0.0387  0.2465  0.0134  0.0687\n",
       " -0.0709  0.1390  0.0607 -0.0733  0.0229\n",
       " -0.0333 -0.3088  0.1943  0.0076 -0.3491\n",
       "  0.1605  0.0167  0.2333  0.0434  0.0548\n",
       " -0.1442 -0.0616 -0.0858  0.0664 -0.0981\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  0.1851  0.2014  0.2626 -0.1695  0.1036\n",
       "  0.1662  0.2452  0.0833 -0.2856  0.2386\n",
       " -0.1514 -0.1682  0.0652 -0.0119  0.1201\n",
       "  0.1846  0.2221 -0.5222  0.0968  0.2813\n",
       " -0.3031 -0.1098  0.0267  0.1611 -0.0775\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -0.2471 -0.1852  0.1820  0.0186 -0.1737\n",
       " -0.2447  0.1666  0.2954  0.0391  0.2407\n",
       "  0.2722  0.2943  0.0079  0.3076 -0.2122\n",
       " -0.4312 -0.1070  0.0499 -0.0747 -0.1921\n",
       " -0.1333  0.2013 -0.1804  0.2001 -0.1866\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       " -0.1762 -0.0049  0.1020 -0.1775 -0.2316\n",
       "  0.0612  0.0491 -0.5485 -0.1238 -0.0899\n",
       "  0.1017 -0.0156 -0.0949 -0.1207 -0.0123\n",
       " -0.2272 -0.1303  0.0264  0.0550 -0.2789\n",
       " -0.1655  0.1447  0.3437  0.0388  0.2036\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       "  0.4420  0.1717 -0.1792 -0.1662 -0.1717\n",
       "  0.2337 -0.1935 -0.1935  0.0074 -0.3848\n",
       "  0.0458  0.0538 -0.1652  0.7083  0.2325\n",
       " -0.1445  0.2670  0.0752  0.2457  0.0058\n",
       " -0.1204  0.0243  0.3467 -0.0673 -0.1843\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.2499  0.2373  0.3118  0.0506  0.2374\n",
       "  0.1394 -0.0573  0.0146  0.1962  0.1444\n",
       " -0.0315 -0.2465  0.0030 -0.1132  0.4107\n",
       "  0.3123 -0.3399  0.1662 -0.1894  0.0508\n",
       "  0.0691 -0.0112 -0.3206 -0.0715 -0.0685\n",
       "[torch.FloatTensor of size 6x1x5x5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MSELossBackward object at 0x1117fc228>\n",
      "<torch.autograd.function.AddmmBackward object at 0x1117fc138>\n",
      "<AccumulateGrad object at 0x1117422b0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be placed in a loop for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
